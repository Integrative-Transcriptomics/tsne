{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc6b9c9-c4f3-4ddf-98b2-4a21f5cfbf58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from functools import partial\n",
    "from jax import vmap\n",
    "from jax.lax import scan\n",
    "from jax.lax import cond\n",
    "from jax import random\n",
    "from jax import jit\n",
    "from jax import jacrev\n",
    "from jax.lax import stop_gradient\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn import manifold, datasets\n",
    "import seaborn as sns\n",
    "\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7950b2-caec-4d75-b1bc-a4e66a8d545f",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6c6365-684c-47e8-97df-8c61af3b0ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MidpointNormalize(mpl.colors.Normalize):\n",
    "    def __init__(self, vmin, vmax, midpoint=0, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        mpl.colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        normalized_min = max(0, 1 / 2 * (1 - abs((self.midpoint - self.vmin) / (self.midpoint - self.vmax))))\n",
    "        normalized_max = min(1, 1 / 2 * (1 + abs((self.vmax - self.midpoint) / (self.midpoint - self.vmin))))\n",
    "        normalized_mid = 0.5\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [normalized_min, normalized_mid, normalized_max]\n",
    "        return onp.ma.masked_array(onp.interp(value, x, y))\n",
    "\n",
    "def pca(X: np.ndarray, no_dims=50):\n",
    "    \"\"\"\n",
    "        Runs PCA on the NxD array X in order to reduce its dimensionality to\n",
    "        no_dims dimensions.\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    #(n, d) = X.shape\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    u, s, vh = np.linalg.svd(X, full_matrices=False, compute_uv=True, hermitian=False)\n",
    "    #l = (s ** 2 / (d))[0:no_dims]\n",
    "    M = vh[0:no_dims, :]\n",
    "    Y = np.dot(X, M.T)\n",
    "    return Y\n",
    "\n",
    "def Hbeta(D: np.ndarray, beta=1.0):\n",
    "    \"\"\"\n",
    "    Compute the log2(perplexity)=Entropy and the P-row (P_i) for a specific value of the\n",
    "        precision=1/(sigma**2) (beta) of a Gaussian distribution. D: vector of squared Euclidean distances (without i)\n",
    "    :param D: vector of length d, squared Euclidean distances to all other datapoints (except itself)\n",
    "    :param beta: precision = beta = 1/sigma**2\n",
    "    :return: H: log2(Entropy), P: computed probabilites\n",
    "    \"\"\"\n",
    "    P = np.exp(-D * beta)     # numerator of p j|i\n",
    "    sumP = np.sum(P, axis=None)    # denominator of p j|i --> normalization factor\n",
    "    H = np.log(sumP) + beta * np.sum(D * P) / sumP\n",
    "    P = P / sumP\n",
    "    return H, P\n",
    "\n",
    "def HdiffGreaterTrue(*betas):\n",
    "    beta, betamax = betas\n",
    "    return beta*2\n",
    "\n",
    "def HdiffGreaterFalse(*betas):\n",
    "    beta, betamax = betas\n",
    "    return (beta+betamax)/2\n",
    "\n",
    "def HdiffSmallerTrue(*betas):\n",
    "    beta, betamin = betas\n",
    "    return beta/2\n",
    "\n",
    "def HdiffSmallerFalse(*betas):\n",
    "    beta, betamin = betas\n",
    "    return (beta+betamin)/2\n",
    "\n",
    "def HdiffGreater(*betas):\n",
    "    beta, betamin, betamax = betas\n",
    "    betamin = beta\n",
    "    beta = cond((np.logical_or(betamax == np.inf, betamax == -np.inf)), HdiffGreaterTrue, HdiffGreaterFalse, *(beta, betamax))\n",
    "    return beta, betamin, betamax\n",
    "\n",
    "def HdiffSmaller(*betas):\n",
    "    beta, betamin, betamax = betas\n",
    "    betamax = beta\n",
    "    beta = cond(np.logical_or(betamin == np.inf, betamin == -np.inf), HdiffSmallerTrue, HdiffSmallerFalse, *(beta, betamin))\n",
    "    return beta, betamin, betamax\n",
    "\n",
    "def HdiffGreaterTolerance(*betas):\n",
    "    beta, betamin, betamax, Hdiff = betas\n",
    "    beta, betamin, betamax = cond(Hdiff > 0, HdiffGreater, HdiffSmaller, *(beta, betamin, betamax))\n",
    "    return beta, betamin, betamax, Hdiff\n",
    "\n",
    "def binarySearch(res, el, Di, logU):\n",
    "    print('Entered binary search function')\n",
    "    Hdiff, thisP, beta, betamin, betamax = res\n",
    "    Hdiffbool = np.abs(Hdiff) < 1e-5\n",
    "    beta, betamin, betamax, Hdiff = cond(np.abs(Hdiff) < 1e-5, lambda a, b, c, d: (a, b, c, d), HdiffGreaterTolerance, *(beta, betamin, betamax, Hdiff))\n",
    "\n",
    "    (H, thisP) = Hbeta(Di, beta)\n",
    "    Hdiff = H - logU\n",
    "    return (Hdiff, thisP, beta, betamin, betamax), el\n",
    "\n",
    "def x2p_inner(Di: np.ndarray, iterator, beta, betamin, betamax, perplexity=30, tol=1e-5):\n",
    "    \"\"\"\n",
    "    binary search for precision for Pi such that it matches the perplexity defined by the user\n",
    "    :param Di: vector of length d-1, squared Euclidean distances to all other datapoints (except itself)\n",
    "    :param beta: precision = beta = 1/sigma**2\n",
    "    :return: final probabilites p j|i\n",
    "    \"\"\"\n",
    "    # Compute the Gaussian kernel and entropy for the current precision\n",
    "    logU = np.log(perplexity)\n",
    "    H, thisP = Hbeta(Di, beta)\n",
    "    Hdiff = H - logU\n",
    "\n",
    "\n",
    "\n",
    "    print('Starting binary search')\n",
    "    #Hdiff, thisP, beta, betamin, betamax = binarySearch((Hdiff, None, beta, betamin, betamax), None, Di, logU)\n",
    "    binarySearch_func = partial(binarySearch, Di=Di, logU=logU)\n",
    "\n",
    "    #for i in range(20):\n",
    "    #    (Hdiff, thisP, beta, betamin, betamax), el = binarySearch_func((Hdiff, thisP, beta, betamin, betamax), (Hdiff, thisP, beta, betamin, betamax))\n",
    "\n",
    "\n",
    "    # Note: the following binary Search for suitable precisions (betas) will be repeated 50 times and does not include the threshold value\n",
    "    (Hdiff, thisP, beta, betamin, betamax), el = scan(binarySearch_func, init=(Hdiff, thisP, beta, betamin, betamax), xs=None, length=50)    # Set the final row of P\n",
    "    thisP = np.insert(thisP, iterator, 0)\n",
    "    return thisP\n",
    "\n",
    "def x2p(X: np.ndarray, tol=1e-5, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Performs a binary search to get P-values (high-dim space) in such a way that each\n",
    "        conditional Gaussian has the same perplexity.\n",
    "    \"\"\"\n",
    "    # Initialize some variables\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    (n, d) = X.shape\n",
    "    sum_X = np.sum(np.square(X), 1)\n",
    "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
    "    D = np.reshape(np.delete(D, np.array([i for i in range(0, D.shape[0]**2, (D.shape[0]+1))])), (n , n - 1 ))\n",
    "    beta = np.ones(n)      # precisions (1/sigma**2)\n",
    "    betamin = np.full(n, -np.inf)\n",
    "    betamax = np.full(n, np.inf)\n",
    "    P = vmap(partial(x2p_inner, perplexity=perplexity, tol=tol))(D, np.arange(n), beta=beta, betamin=betamin, betamax=betamax)\n",
    "    return P\n",
    "\n",
    "\n",
    "def optimizeY(res, el, P, initial_momentum=0.5, final_momentum=0.8, eta=500, min_gain=0.01):\n",
    "    Y, iY, gains, i = res\n",
    "    n, d = Y.shape\n",
    "\n",
    "    # Compute pairwise affinities\n",
    "    sum_Y = np.sum(np.square(Y), 1)\n",
    "    num = -2. * np.dot(Y, Y.T)  # numerator\n",
    "    num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))\n",
    "    num = num.at[np.diag_indices_from(num)].set(0.)     # numerator\n",
    "    Q = num / np.sum(num)\n",
    "    Q = np.maximum(Q, 1e-12)\n",
    "\n",
    "\n",
    "    # Compute gradient\n",
    "    PQ = P - Q\n",
    "    PQ_exp = np.expand_dims(PQ, 2)  # NxNx1\n",
    "    Y_diffs = np.expand_dims(Y, 1) - np.expand_dims(Y, 0)  # nx1x2 - 1xnx2= # NxNx2\n",
    "    num_exp = np.expand_dims(num, 2)    # NxNx1\n",
    "    Y_diffs_wt = Y_diffs * num_exp\n",
    "    grad = np.sum((PQ_exp * Y_diffs_wt), axis=1) # Nx2\n",
    "\n",
    "    # Update Y\n",
    "    momentum = cond(i<20, lambda: initial_momentum, lambda: final_momentum)\n",
    "    # this business with \"gains\" is the bar-delta-bar heuristic to accelerate gradient descent\n",
    "    # code could be simplified by just omitting it\n",
    "    # inc = iY * grad > 0\n",
    "    # dec = iY * grad < 0\n",
    "    # gains = np.where((iY * grad > 0), gains+0.2, gains)\n",
    "    # gains = np.where((iY * grad < 0), gains*0.8, gains)\n",
    "    gains = np.clip(gains, min_gain, np.inf)\n",
    "\n",
    "    iY = momentum * iY - eta * (gains * grad)\n",
    "    Y = Y + iY\n",
    "\n",
    "    Y = Y - np.mean(Y, axis=0)\n",
    "    P = cond(i==100, lambda x: x/4., lambda x:x, P)\n",
    "    i += 1\n",
    "    return ((Y, iY, gains, i), 1.0)\n",
    "\n",
    "def optimizeYforBackprob(res, el, P, initial_momentum=0.5, final_momentum=0.8, eta=500, min_gain=0.01):\n",
    "    Y, iY, gains, i = res\n",
    "    n, d = Y.shape\n",
    "\n",
    "    # Compute pairwise affinities\n",
    "    sum_Y = stop_gradient(np.sum(np.square(Y), 1))\n",
    "    num = stop_gradient(-2. * np.dot(Y, Y.T))  # numerator\n",
    "    num = stop_gradient(1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y)))\n",
    "    num = stop_gradient(num.at[np.diag_indices_from(num)].set(0.))     # numerator\n",
    "    Q = stop_gradient(num / np.sum(num))\n",
    "    Q = stop_gradient(np.maximum(Q, 1e-12))\n",
    "\n",
    "\n",
    "    # Compute gradient\n",
    "    PQ = P - Q\n",
    "    PQ_exp = np.expand_dims(PQ, 2)  # NxNx1\n",
    "    Y_diffs = stop_gradient(np.expand_dims(Y, 1) - np.expand_dims(Y, 0))  # nx1x2 - 1xnx2= # NxNx2\n",
    "    num_exp = np.expand_dims(num, 2)    # NxNx1\n",
    "    Y_diffs_wt = stop_gradient(Y_diffs * num_exp)\n",
    "    grad = np.sum((PQ_exp * Y_diffs_wt), axis=1) # Nx2\n",
    "\n",
    "    # Update Y\n",
    "    momentum = cond(i<20, lambda: initial_momentum, lambda: final_momentum)\n",
    "    # this business with \"gains\" is the bar-delta-bar heuristic to accelerate gradient descent\n",
    "    # code could be simplified by just omitting it\n",
    "    # inc = iY * grad > 0\n",
    "    # dec = iY * grad < 0\n",
    "    # gains = np.where((iY * grad > 0), gains+0.2, gains)\n",
    "    # gains = np.where((iY * grad < 0), gains*0.8, gains)\n",
    "    gains = np.clip(gains, min_gain, np.inf)\n",
    "\n",
    "    iY = momentum * iY - eta * (gains * grad)\n",
    "    Y = Y + iY\n",
    "\n",
    "    Y = Y - np.mean(Y, axis=0)\n",
    "    P = cond(i==100, lambda x: x/4., lambda x:x, P)\n",
    "    i += 1\n",
    "    return ((Y, iY, gains, i), 1.0)\n",
    "\n",
    "def tsne(X: np.ndarray, no_dims=2, initial_dims=50, perplexity=30.0, learning_rate=500, max_iter = 1000):\n",
    "    \"\"\"\n",
    "        Runs t-SNE on the dataset in the NxD array X to reduce its\n",
    "        dimensionality to no_dims dimensions. The syntaxis of the function is\n",
    "        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check inputs\n",
    "    if isinstance(no_dims, float):\n",
    "        print(\"Error: array X should have type float.\")\n",
    "        return -1\n",
    "    if round(no_dims) != no_dims:\n",
    "        print(\"Error: number of dimensions should be an integer.\")\n",
    "        return -1\n",
    "\n",
    "    X = pca(X, initial_dims)\n",
    "    (n, d) = X.shape\n",
    "    key = random.PRNGKey(42)\n",
    "\n",
    "    initial_momentum = 0.5\n",
    "    final_momentum = 0.8\n",
    "    eta = learning_rate   # Initial learning rate\n",
    "    min_gain = 0.01\n",
    "    # Initialize solution\n",
    "    Y = random.normal(key, shape=(n, no_dims))\n",
    "    dY = np.zeros((n, no_dims))\n",
    "    #Y_t1 = np.zeros((n, no_dims))\n",
    "    #Y_t2 = np.zeros((n, no_dims))\n",
    "    iY = np.zeros((n, no_dims))\n",
    "    gains = np.ones((n, no_dims))\n",
    "\n",
    "    # Compute P-values\n",
    "    P = x2p(X, 1e-5, perplexity)    # I don't know if the computed P is correct np.sum(P, axis=0) is not 1 everywhere\n",
    "    P = (P + np.transpose(P))\n",
    "\n",
    "    P = P / np.sum(P)      # Why don't we devide by 2N as described everywhere?\n",
    "    P = P * 4.  # early exaggeration\n",
    "    P = np.maximum(P, 1e-12)\n",
    "\n",
    "    # for debugging\n",
    "    #for i in range(1000):\n",
    "    #  ((P, Y, dY, iY, gains, i), j) = optimizeY((P, Y, dY, iY, gains, i), el=1, initial_momentum = initial_momentum, final_momentum = final_momentum, eta = eta, min_gain = min_gain)\n",
    "\n",
    "\n",
    "    # jit-compiled version\n",
    "    optimizeY_func = partial(optimizeYforBackprob, P=P, initial_momentum = initial_momentum, final_momentum = final_momentum, eta = eta, min_gain = min_gain)\n",
    "    ((Y, iY, gains, i), el) = scan(optimizeY_func, init=(Y, iY, gains, 0), xs=None, length=max_iter)  # Set the final row of P\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137b88b-dc72-4120-869f-065af87575c0",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d20e3-cdc2-4ee8-8302-f9460fceb358",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the data using PCA...\n",
      "Computing pairwise distances...\n",
      "Starting binary search\n",
      "Entered binary search function\n",
      "Preprocessing the data using PCA...\n",
      "Computing pairwise distances...\n",
      "Starting binary search\n",
      "Entered binary search function\n"
     ]
    }
   ],
   "source": [
    "n, p = (800, 10)\n",
    "X, y = datasets.make_blobs(n, p, cluster_std=0.3)\n",
    "tsne_func = partial(tsne, no_dims=2, initial_dims=50, perplexity=20.0, learning_rate=400, max_iter=50)\n",
    "Y = tsne_func(X)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=y)\n",
    "tsne_jacobian = jacrev(tsne_func)(X)\n",
    "print('mean derivative', np.mean(tsne_jacobian))\n",
    "print('median derivative', np.median(tsne_jacobian))\n",
    "print('Jacobian shape', tsne_jacobian.shape)\n",
    "jacobian_reshape = np.reshape(np.reshape(tsne_jacobian, (2, n, n*p), order='F'),\n",
    "                                        (n*2,\n",
    "                                        n*p))\n",
    "vmin=onp.min(jacobian_reshape)\n",
    "vmax=onp.max(jacobian_reshape)\n",
    "f = plt.figure(figsize=(30, 10))\n",
    "ax = sns.heatmap(jacobian_reshape, cmap=\"coolwarm\", norm=(MidpointNormalize(midpoint=0, vmin=vmin, vmax=vmax)))\n",
    "ax.set_xlabel('Input points X vectorized')\n",
    "ax.set_ylabel('Output points Y vectorized')\n",
    "ax.set_title('Jacobian of tsne w.r.t input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e133426e-54f9-454e-bcdb-c02f9df1695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian_sum_per_sample = []\n",
    "jacobian_sum = np.sum(np.abs(jacobian_reshape), axis=0)\n",
    "for i in range(n):\n",
    "    sum_for_sample = []\n",
    "    for j in range(p):\n",
    "        sum_for_sample.append(jacobian_sum[j*n+i])\n",
    "    jacobian_sum_per_sample.append(sum(sum_for_sample))\n",
    "len(jacobian_sum_per_sample)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=jacobian_sum_per_sample, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817ae0d-a8b6-4df0-82a4-7d06c86b15f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax = plt.hist(jacobian_reshape.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b046a53-370e-49fe-906a-c540b7de0d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b42c6c-7f15-4a41-8752-b409b760eaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
